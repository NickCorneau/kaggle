{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Handwritten Digits using Neural Networks\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "This is a program that will learn how to **_recognize handwritten digits_** using **_stochastic gradient descent_** and the **_MNIST training data_** found [here](https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network\n",
    "\n",
    "The centerpiece is a 'Network' class, which we use to represent a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__ (self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sizes`** contains the number of neurons in the respective layers. \n",
    "\n",
    "**`biases`** and **`weights`** are randomly initialized. This gives our stochastic gradient descent algorithm a starting place.\n",
    "\n",
    "To a create **`Network`** object with 784 neurons in the first layer, 15 neurons in the second layer and 10 neurons in the output layer, we'd do this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([784, 15, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Computing the output of our Network\n",
    "\n",
    "With this in mind, it's easy to write code to begin computing the output of the **`Network`** class. Let **`σ`** be the **`sigmoid function`**:\n",
    "\n",
    "$$σ(z)≡\\frac{1}{1+e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.228924  ,  1.04213608,  0.32398541, -0.77347495, -1.06165523,\n",
       "        0.88446953,  0.15387377,  0.84646595, -0.25830617, -1.05593415,\n",
       "       -0.55326975,  0.35402689,  0.3477687 ,  0.67332864,  1.39453236])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`net.weights[1]` denotes the weights connecting the second and third layer of the network. Let's call that matrix **`w`** for now. Let **`a`** be the vector of activations of the second layer of neurons and let **`b`** be the vector of biases. Let **`a′`** be the **`feed forward function`**:\n",
    "\n",
    "$$a′ =σ(wa+b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want a way to apply this `feedforward` method. Let's do this using **[stochastic gradient descent](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html)**.\n",
    "\n",
    "The idea is to use gradient descent to find the weights w<sub>k</sub> and biases b<sub>l</sub> which minize the cost function such that the output from the network approximates y(x) for all training inputs x. In other words, our \"position\" now has components w<sub>k</sub> and b<sub>l</sub>, and the gradient vector ∇C has corresponding components ∂C/∂w<sub>k</sub> and ∂C/∂b<sub>l</sub>. Writing out the gradient descent update rule in terms of components, we have\n",
    "\n",
    "$$w_k \\rightarrow w_k' = w_k-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}\\\\$$\n",
    "\n",
    "$$ b_l  \\rightarrow  b_l' = b_l-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l}\\\\$$\n",
    "  \n",
    "By repeatedly applying this update rule we can \"roll down the hill\", and hopefully find a minimum of the cost function. In other words, this is a rule which can be used to learn in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    if test_data: n_test = len(test_data)\n",
    "    n = len(training_data)\n",
    "    for j in xrange(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                        for k in xrange(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        if test_data:\n",
    "            print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "        el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:digits]",
   "language": "python",
   "name": "conda-env-digits-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
